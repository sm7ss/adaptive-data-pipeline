# ğŸš€ Adaptive Data Pipeline

[![Ray](https://img.shields.io/badge/Ray-Distributed__Computing-blue.svg)](https://ray.io/)
[![Polars](https://img.shields.io/badge/Polars-Fast__DataFrames-red.svg)](https://pola.rs/)
[![Pydantic](https://img.shields.io/badge/Pydantic-Data__Validation-green.svg)](https://pydantic-docs.helpmanual.io/)
[![Python](https://img.shields.io/badge/Python-3.8+-yellow.svg)](https://www.python.org/)
[![Architecture](https://img.shields.io/badge/Architecture-Adaptive-orange.svg)](#)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

Un sistema de procesamiento de datos **auto-adaptativo** que selecciona inteligentemente entre modos de ejecuciÃ³n (Eager/Lazy/Ray) basado en el tamaÃ±o de los datos y recursos disponibles, integrando Polars para mÃ¡ximo rendimiento y Pydantic para validaciÃ³n robusta.

## ğŸŒŸ CaracterÃ­sticas Principales

### ğŸ§  **Inteligencia Adaptativa**
- **SelecciÃ³n automÃ¡tica** del modo de procesamiento Ã³ptimo
- **GestiÃ³n inteligente de memoria** y recursos
- **Decision-making en tiempo real** basado en mÃ©tricas del sistema

### âš¡ **Multi-Modal Processing**
- **Eager Mode**: Para datasets pequeÃ±os (Polars DataFrame)
- **Lazy Mode**: Para datasets medianos (Polars LazyFrame)  
- **Distributed Mode**: Para big data (Ray Cluster)

### ğŸ›¡ï¸ **Robustez Empresarial**
- **ValidaciÃ³n completa** de schemas y configuraciones
- **Manejo automÃ¡tico de errores** y recuperaciÃ³n
- **Soporte mÃºltiples formatos** (YAML/TOML)


## ğŸ—ï¸ Arquitectura del Sistema

adaptive-data-pipeline/
â”œâ”€â”€ ğŸ“ core/ # NÃºcleo del sistema
â”‚ â”œâ”€â”€ PipelineDataPreProcessing.py # Orquestador principal
â”‚ â””â”€â”€ Frame.py # Gestor de frames adaptativo
â”œâ”€â”€ ğŸ“ operations/ # Operaciones de procesamiento
â”‚ â”œâ”€â”€ RayOperations.py # Procesamiento distribuido
â”‚ â””â”€â”€ PolarsOperations.py # Operaciones Polars
â”œâ”€â”€ ğŸ“ validation/ # ValidaciÃ³n y configuraciÃ³n
â”‚ â”œâ”€â”€ Validator.py # ValidaciÃ³n de schemas
â”‚ â””â”€â”€ ReadData.py # Lectura de configuraciones
â”œâ”€â”€ ğŸ“ resources/ # GestiÃ³n de recursos
â”‚ â””â”€â”€ Resources.py # OptimizaciÃ³n de recursos Ray
â””â”€â”€ ğŸ“ strategies/ # Estrategias y patrones
â””â”€â”€ Strategy.py # Enums y estrategias

## ğŸ¯ Toma de Decisiones Inteligente

### **Algoritmo de SelecciÃ³n de Modo**

```python
def frame_decision(self, tamaÃ±o: int, memory: int) -> str:
    umbral_eager = 0.1 * memory    # 10% de RAM disponible
    umbral_lazy = 0.75 * memory    # 75% de RAM disponible
    
    if tamaÃ±o < umbral_eager:
        return 'eager'    # Polars DataFrame
    elif tamaÃ±o < umbral_lazy:
        return 'lazy'     # Polars LazyFrame  
    else:
        return 'ray'      # Ray Distributed
```

### **GestiÃ³n AutomÃ¡tica de Recursos**

```python 
def ray_init(tamaÃ±o_archivo: float) -> Dict[str, Any]:
    memory = psutil.virtual_memory().available
    cpus = psutil.cpu_count(logical=False)
    
    num_cpus = max(1, cpus - 1)  # Reservar 1 CPU para el sistema
    object_store_memory = max(memory * 0.3, tamaÃ±o_archivo * 1.5)
    
    return {
        'num_cpus': num_cpus,
        'object_store_memory': object_store_memory
    }
```

## ğŸš€ Ejemplos de Uso

### **ConfiguraciÃ³n BÃ¡sica (YAML)**

```yaml 
data:
  input_path: ['data/users.csv', 'data/transactions.csv']
  output_path: 'processed_data.parquet'

clean_data:
  operation: 'drop_nulls'
  col: 'age'

window_data:
  operation: 'rolling_mean'
  col: 'value'
  window: 5
  over: 'user_id'

join_data:
  join_on: 'user_id'
  join_type: 'inner'
  post_filter:
    operations: ['value > 100']
```

### **EjecuciÃ³n AutomÃ¡tica**

```python 
from src.core.PipelineDataPreProcessing import Pipeline

# El sistema elige automÃ¡ticamente el mejor modo de procesamiento
pipeline = Pipeline('config.yaml')
pipeline.operaciones_frame()

# SegÃºn el tamaÃ±o de datos y recursos, ejecutarÃ¡:
# - Polars Eager (datasets pequeÃ±os)
# - Polars Lazy (datasets medianos) 
# - Ray Distributed (big data)
```

## ğŸ”§ Operaciones Soportadas

### ğŸ“Š **Transformaciones de Datos**

- Limpieza: Drop nulls, filtrado por condiciones
- Ventanas MÃ³viles: Rolling means, sums, min/max
- Agregaciones: GroupBy con mÃºltiples operaciones
- Joins: Inner, left, right, outer con post-filtrado

### âš¡ **Modos de EjecuciÃ³n**

| Modo	          | Caso de Uso	       | TecnologÃ­a       |
|-----------------|--------------------|------------------|
| **Eager**	      | Datasets < 10% RAM | Polars DataFrame |
| **Lazy**	      | Datasets < 75% RAM | Polars LazyFrame |
| **Distributed** |	Big Data > 75% RAM | Ray Cluster      |

### ğŸ›¡ï¸ **Validaciones**

- Schemas: VerificaciÃ³n de columnas y tipos
- Recursos: ValidaciÃ³n de memoria y CPU disponibles
- ConfiguraciÃ³n: Sintaxis YAML/TOML y constraints

## ğŸ“¦ InstalaciÃ³n

### **Requisitos**

```bash 
# requirements.txt
ray>=2.0.0
polars>=0.19.0
pydantic>=2.0.0
pyyaml>=6.0
tomli>=2.0.0
psutil>=5.9.0
pyarrow>=12.0.0
```

### **InstalaciÃ³n Completa**

```bash
git clone https://github.com/sm7ss/adaptive-data-pipeline.git
cd adaptive-data-pipeline

# InstalaciÃ³n en desarrollo
pip install -e .

# O instalar dependencias directamente
pip install -r requirements.txt
```

## ğŸª Flujo de Procesamiento

### **1. AnÃ¡lisis de Datos**

```python 
# El sistema analiza tamaÃ±o de datos y recursos
tamaÃ±o = archivo.stat().st_size
memory = psutil.virtual_memory().available
decision = self.frame_decision(tamaÃ±o, memory)
```

### **2. SelecciÃ³n de TecnologÃ­a**

```python 
if decision == 'eager':
    frame = PolarsFrame.leer_eager(path)
elif decision == 'lazy':
    frame = PolarsFrame.leer_lazy(path)  
elif decision == 'ray':
    frame = RayFrame.leer_ray(path)
```

### **3. Procesamiento Optimizado**

```python 
# AplicaciÃ³n de transformaciones segÃºn configuraciÃ³n
df_transform = frame.with_columns(self.expr.list_expr())

# Operaciones complejas (joins, agrupaciones)
if necesita_join:
    resultado = self.group_join.join_data(frame1, frame2)
```

### **4. Escritura de Resultados**

```python
# Guardado optimizado segÃºn el modo
if es_eager:
    resultado.write_parquet(output_path)
else:
    resultado.sink_parquet(output_path, compression='zstd')
```

## ğŸ” Casos de Uso

### ğŸª **E-commerce Analytics**

```yaml
data:
  input_path: ['users.csv', 'purchases.csv']
  output_path: 'customer_analytics.parquet'

window_data:
  operation: 'rolling_sum'
  col: 'purchase_amount'
  window: 30
  over: 'customer_id'

join_data:
  join_on: 'customer_id'
  join_type: 'left'
```

### ğŸ¥ **Healthcare Processing**

```yaml
data:
  input_path: 'patient_records.csv'
  output_path: 'medical_features.parquet'

clean_data:
  operation: 'drop_nulls'
  col: 'blood_pressure'

agg_data:
  operation: 'mean'
  col: 'heart_rate'
  group_by: 'patient_group'
```

### ğŸ“± **Telecomm Analytics**

```yaml
data:
  input_path: ['customers.csv', 'usage_data.csv']
  output_path: 'telecom_features.parquet'

window_data:
  operation: 'rolling_mean'
  col: 'data_usage'
  window: 7
  over: 'user_id'

join_data:
  join_on: 'user_id'
  join_type: 'inner'
  post_filter:
    operations: ['data_usage > 5000']
```

## ğŸ¤ ContribuciÃ³n

Â¡Contribuciones son bienvenidas! Este proyecto utiliza arquitectura modular:

1. Fork el proyecto
2. Crea una rama (git checkout -b feature/nueva-operacion)
3. Commit cambios (git commit -m 'Agregar nueva operaciÃ³n de ventana')
4. Push a la rama (git push origin feature/nueva-operacion)
5. Abre un Pull Request

## ğŸ‘©â€ğŸ’» Sobre el Proyecto

**Adaptive Data Pipeline** representa la culminaciÃ³n de mi journey en data engineering, combinando tÃ©cnicas avanzadas de procesamiento distribuido con inteligencia artificial aplicada a la gestiÃ³n de recursos.

El sistema demuestra cÃ³mo la automatizaciÃ³n inteligente puede optimizar pipelines de datos complejos, seleccionando la tecnologÃ­a Ã³ptima para cada escenario sin intervenciÃ³n manual.

**Â¿Preguntas tÃ©cnicas?** Â¡No dudes en abrir un issue!

